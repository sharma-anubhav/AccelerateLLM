# â±ï¸ SpeculativeÂ Decoding
This repository offers a complete, reference implementation of **speculative decoding** algorithms for **large language models**. It implements two interchangeable strategiesâ€”(1) a **tree-based** approach that merges multi-beam drafts from lightweight speculative models into a single trie and verifies every branch in one forward pass, and (2) a **sequential** draft-verification loop that accepts tokens step-by-step. Alongside the core **Python** code, the repo includes **benchmarking scripts**, extensive **JSON logging**, and a **Streamlit dashboard** so you can explore approach **trade-offs** and understand exactly how speculative decoding works while **LLM inference**.

> **PurposeÂ & Limitations**  
> This repository provides speculativeâ€‘decoding algorithms with a <ins>pureâ€‘Python reference implementation</ins> and visualization tools. It is intended for learning and experimentationâ€”**not** for productionâ€‘grade speedâ€‘ups you would get from a lowâ€‘level (C++/CUDA) backend.


## âœï¸ Introduction

Large Language Models (LLMs) are remarkably capable, yet their autoregressive nature makes inference costly. **Speculative decoding** cuts latency by letting lightweight **speculative models (SSMs)** propose future tokens that a heavyweight **target model** verifies.  
This repo offers two interchangeable strategies:

- **Treeâ€‘based attention** â€“ multiâ€‘beam drafts are merged into a Trie and verified in _one_ forward pass.
- **Sequential draftâ€‘verification** â€“ classic tokenâ€‘byâ€‘token acceptance loop for baselines and ablations.

Everything is implemented with PyTorchÂ +Â Transformersâ€”no custom kernels required.

---

## ğŸ—ºï¸ TableÂ ofÂ Contents

1. [DirectoryÂ Structure](#1-directory-structure)
2. [HowÂ toÂ Run](#2-how-to-run)
3. [Benchmarking](#3-benchmarking)
4. [Highlights](#4-highlights)
5. [Methodology](#5-methodology)
6. [ExampleÂ Prompt](#6-example-prompt)
7. [FutureÂ Optimizations](#7-future-optimizations)

---

## 1. ğŸ—‚ï¸ DirectoryÂ Structure

```text
project_root/
â”œâ”€â”€ config.py                # Configurable prompts, models, decoding mode, beam params
â”œâ”€â”€ main.py                  # CLI entry for singleâ€‘prompt runs
â”œâ”€â”€ run_benchmark.py         # Batch benchmark runner
â”œâ”€â”€ controller.py            # Sequential speculative controller
â”œâ”€â”€ controller_tree.py       # Treeâ€‘based speculative controller
â”œâ”€â”€ sequential_utils.py      # Drafter & verifier helpers
â”œâ”€â”€ tree_utils.py            # Trie + treeâ€‘mask utilities
â”œâ”€â”€ model_loader.py          # CUDAâ€‘aware model loader
â”œâ”€â”€ benchmarking/
â”‚   â”œâ”€â”€ analyze.py           # Aggregate log metrics
â”‚   â”œâ”€â”€ visualize.py         # Streamlit dashboard
â”‚   â”œâ”€â”€ benchmark_summary.csv
â”‚   â””â”€â”€ config.py            # Benchmark parameters
â”œâ”€â”€ logs/                    # JSON logs per run
â””â”€â”€ README.md                # You are here
```

---

## 2. HowÂ toÂ Run

### SingleÂ Prompt viaÂ CLI

```bash
python main.py \
  --prompt "The future of AI is" \
  --mode tree            # or sequential
  --draft_length 9 \
  --max_tokens 40 \
  --verbose
```

`--mode` options:

- `sequential`Â â€“ draftÂ +Â verify in rounds
- `tree`Â â€“ multiâ€‘beam drafts, one target pass with tree mask

---

## 3. Benchmarking

Run a full sweep across prompts and configs:

```bash
python benchmarking/run_benchmark.py
```

Key grid in `benchmarking/config.py`:

```python
CONFIG = {
    "target_model": "Qwen/Qwen2.5-3B",
    "draft_models": ["Qwen/Qwen2.5-0.5B", "Qwen/Qwen2.5-1.5B"],
    "max_new_tokens_list": [40, 80, 120],
    "sequential_draft_lengths": [2, 4, 6, 8, 10],
    "tree_beam_widths": [1, 2, 3, 5, 6],
    "tree_beam_depths": [2, 4, 6, 8, 10],
    "prompts": [
        "Once upon a time",
        "The future of AI is",
        "The president of the USA is",
        "In a galaxy far away",
    ],
}
```

### ğŸ“Š InteractiveÂ Dashboard

```bash
streamlit run benchmarking/visualize.py
```

Features:

- Iteration by Iteration visualization of generated, accepted, and correction tokens
- Interactive table filtering (Prompt, Strategy, Configs)
- Scatter and stacked bar plots for performance analysis
- Aggregated statistics across strategies
- Best performing tree, sequential configurations
- Comparison across different target-draft model setups

### Visualizations

<span style="color:red">Red tokens</span> are correction tokens added by the target, while <span style="color:green">green tokens</span> are the accepted tokens generated by the draft model.

![Generation Visualization](images/generation.png)

<br>

Scatter and stacked bar plots for performance analysis, and aggregated statistics across different strategies.

![Token Generation Statistics](images/statistics.png)

---

## 4. âœ¨ Highlights

- **Swap controllers** instantly with a flag.
- **Trieâ€‘merged token tree** + custom attention â‡’ single verification pass.
- **Parallel multiâ€‘draft generation** for extra speed.
- **Rich JSON logs** capturing tokenâ€‘level decisions.
- **Streamlit dashboard** for exploratory analysis.

---

## 5. Methodology

### 5.1 System Architecture

<p align="center"><img src="https://github.com/sharma-anubhav/SpeculativeDecoding/blob/main/images/system_design.png?raw=true" alt="System Architecture Diagram" width="650"/></p>

### 5.2 Treeâ€‘Based SpeculativeÂ Decoding

1. **PromptÂ Encoding** â€“ tokenize prompt â†’ `current_ids`
2. **SpeculativeÂ BeamÂ Generation** â€“ each SSM runs beam search (`beam_width Ã— beam_depth`).
3. **BeamÂ Merging** â€“ deduplicate & merge beams into a Trie.
4. **CustomÂ AttentionÂ Mask** â€“ each node attends to prompt + ancestors; verify all nodes in **one** target pass.
5. **Verification** â€“ accept matching paths; on mismatch append bonus token.
6. **Fallback** â€“ greedy target step if first token fails or tree empty.
7. **Iterate** until EOS or `max_tokens`.

### 5.3 SequentialÂ SpeculativeÂ Decoding

1. **PromptÂ Init** â€“ tokenize prompt.
2. **Parallel Drafts** â€“ each SSM generates a fixedâ€‘length greedy continuation.
3. **Tokenâ€‘Level Verification** â€“ compare each draft token to target logits.
4. **SelectÂ BestÂ Draft** â€“ longest verified prefix wins.
5. **Correction** â€“ if none fully match, append target token.
6. **Iterate** until EOS or `max_tokens`.

### 5.4 StrategyÂ Comparison

| Aspect           | Treeâ€‘Based                      | Sequential                          |
| ---------------- | ------------------------------- | ----------------------------------- |
| Draft generation | Multiâ€‘beam per SSM â†’ token tree | Greedy sequence per draft (threads) |
| Verification     | **One** batched target pass     | Tokenâ€‘byâ€‘token passes               |
| Acceptance       | Accept contiguous branch        | Accept prefix until mismatch        |
| Fallback         | Target bonus token              | Greedy target token                 |
| Search space     | Parallel branches               | Single branch/step                  |
| Next draft       | Highestâ€‘matching path           | Draft with longest prefix           |


### 5.5 MetricsÂ &Â Logging

Every run (treeÂ or sequential) logs a single JSON with all or subset of the following:

| Metric Key | Description |
|------------|-------------|
| `tokens_generated` | Final length of the generated continuation |
| `draft_tokens_proposed` | Speculative tokens produced by all SSMs |
| `draft_tokens_accepted` | Tokens verified & accepted without modification |
| `corrections_by_target` | Tokens *added* by the target model when drafts mismatch |
| `percent_tokens_saved` | `(draft_tokens_accepted / tokens_generated) Ã— 100` â€“ rough efficiency measure |
| `verification_calls` | How many forward passes on the target model were made |
| `max_new_tokens` | Upper cap on tokens the run was allowed to generate |
| `draft_length` | Perâ€‘draft speculative length (sequential) *or* `beam_depth` (tree) |
| `beam_width` | Number of beams per SSM (tree mode only) |
| `beam_depth` | Depth of each beam sequence (tree mode only) |
| `tree_nodes_total` | Total nodes in the merged token trie (tree mode) |
| `tree_accepted_tokens` | Tokens validated via tree verification (tree mode) |

Logs are saved to the `logs/` directory and power the Streamlit dashboard.

---

## 6. ExampleÂ Prompt

```python
prompt = "The future of AI is"
```

Sample continuation:

```
â€¦The future of AI is in the hands of the people who build it. The people who build it are the people who will use it. The people who will use it are the people who will be affected by itâ€¦
```

---

## 7. ğŸ’¡ FutureÂ Optimizations

- Reuse **KVâ€‘cache** instead of reâ€‘encoding prompt.
- Dynamic tree depth based on live acceptance rate.
- Quantized / distilled drafts integration.

---

> **License**Â Apacheâ€‘2.0 Â· **Author**Â AnubhavÂ Sharma Â· <sub>UpdatedÂ AprÂ 2025</sub>
